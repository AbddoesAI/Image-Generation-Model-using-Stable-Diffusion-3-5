{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb4f9103"
      },
      "source": [
        "## 1.1 GPU & VRAM Detection\n",
        "\n",
        "First, we verify the GPU and available VRAM to determine optimal training settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b527529"
      },
      "source": [
        "import subprocess\n",
        "import torch\n",
        "\n",
        "def get_gpu_info():\n",
        "    \"\"\"Detect GPU and available VRAM\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"âœ“ GPU Detected: {gpu_name}\")\n",
        "        print(f\"âœ“ Total VRAM: {vram_gb:.2f} GB\")\n",
        "\n",
        "        # Determine recommended settings\n",
        "        if vram_gb < 16:\n",
        "            print(\"âš  Low VRAM detected. Will use aggressive optimizations.\")\n",
        "            return \"low\"\n",
        "        elif vram_gb < 24:\n",
        "            print(\"âœ“ Medium VRAM. Standard LoRA training recommended.\")\n",
        "            return \"medium\"\n",
        "        else:\n",
        "            print(\"âœ“ High VRAM. Full fine-tuning possible.\")\n",
        "            return \"high\"\n",
        "    else:\n",
        "        print(\"âœ— No GPU detected. This notebook requires a GPU.\")\n",
        "        return None\n",
        "\n",
        "vram_tier = get_gpu_info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a7bcba1"
      },
      "source": [
        "## 1.2 Dependency Installation\n",
        "\n",
        "Installing exact versions for reproducibility and compatibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6810892b"
      },
      "source": [
        "# Install core dependencies\n",
        "!pip install -q torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q diffusers[torch]==0.30.3\n",
        "!pip install -q transformers==4.44.2\n",
        "!pip install -q accelerate==0.34.2\n",
        "!pip install -q xformers==0.0.22.post7\n",
        "!pip install -q bitsandbytes==0.43.3\n",
        "!pip install -q peft==0.12.0\n",
        "!pip install -q datasets==2.20.0\n",
        "!pip install -q Pillow==10.4.0\n",
        "!pip install -q safetensors==0.4.4\n",
        "\n",
        "print(\"âœ“ All dependencies installed successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5583694"
      },
      "source": [
        "## 1.3 Configuration Setup\n",
        "\n",
        "Set up Hugging Face authentication and global configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d79da695"
      },
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN_HERE\"  # Replace with your token\n",
        "MODEL_ID = \"stabilityai/stable-diffusion-3.5-large\"  # or stable-diffusion-3.5-medium\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/sd35_training\"  # Google Drive path\n",
        "DATASET_NAME = \"lambdalabs/naruto-blip-captions\"  # Example dataset\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "if HF_TOKEN != \"YOUR_HUGGINGFACE_TOKEN_HERE\":\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"âœ“ Authenticated with Hugging Face\")\n",
        "else:\n",
        "    print(\"âš  Please set your HF_TOKEN to download gated models\")\n",
        "\n",
        "# Mount Google Drive for checkpoint saving\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"âœ“ Output directory: {OUTPUT_DIR}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90965f6e"
      },
      "source": [
        "## 2. Stable Diffusion 3.5 Architecture Overview\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "**1. Multimodal Diffusion Transformer (MMDiT)**\n",
        "- Replaces traditional UNet with a transformer-based architecture\n",
        "- Uses separate pathways for text and image features\n",
        "- Bidirectional information flow between modalities\n",
        "- More parameter-efficient than SDXL\n",
        "\n",
        "**2. Three Text Encoders**\n",
        "- CLIP ViT-L/14: Visual-semantic alignment\n",
        "- CLIP ViT-G/14: Enhanced semantic understanding\n",
        "- T5-XXL: Superior text comprehension and prompt adherence\n",
        "\n",
        "**3. Rectified Flow Framework**\n",
        "- Replaces traditional diffusion process\n",
        "- Straight-line trajectories in latent space (vs. curved diffusion paths)\n",
        "- Fewer sampling steps required (4-8 vs. 20-50)\n",
        "- More stable training dynamics\n",
        "\n",
        "**4. Variational Autoencoder (VAE)**\n",
        "- 16-channel latent space (vs. 4 in SDXL)\n",
        "- Higher fidelity reconstruction\n",
        "- Better detail preservation\n",
        "\n",
        "### SD3.5 vs SDXL:\n",
        "\n",
        "| Feature | SDXL | SD3.5 |\n",
        "|---------|------|-------|\n",
        "| Architecture | UNet | Transformer (MMDiT) |\n",
        "| Text Encoders | 2 (CLIP-L, CLIP-G) | 3 (+T5-XXL) |\n",
        "| Sampling Steps | 20-50 | 4-8 |\n",
        "| Latent Channels | 4 | 16 |\n",
        "| Prompt Adherence | Good | Excellent |\n",
        "| Fine Detail | Good | Superior |\n",
        "\n",
        "### Why SD3.5 for Fine-tuning:\n",
        "\n",
        "1. **Better Prompt Understanding**: T5-XXL encoder enables complex prompt following\n",
        "2. **Faster Convergence**: Rectified flow training is more stable\n",
        "3. **Modular Fine-tuning**: Can target specific components (text encoders, MMDiT)\n",
        "4. **LoRA-Friendly**: Transformer architecture benefits greatly from LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "271833d6"
      },
      "source": [
        "## 3.1 Dataset Loading Strategies\n",
        "\n",
        "Three options for loading training data in Colab:\n",
        "1. **Hugging Face Datasets** (recommended for public datasets)\n",
        "2. **Google Drive** (for private datasets)\n",
        "3. **Direct Upload** (for small datasets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba4e2f0d"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "class SD35Dataset(Dataset):\n",
        "    \"\"\"Custom dataset for SD3.5 training with flexible loading\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_source, resolution=1024, center_crop=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset_source: HF dataset name, Drive path, or list of image paths\n",
        "            resolution: Target image resolution (will bucket nearby sizes)\n",
        "            center_crop: Whether to center crop images\n",
        "        \"\"\"\n",
        "        self.resolution = resolution\n",
        "        self.center_crop = center_crop\n",
        "\n",
        "        # Load dataset based on source type\n",
        "        if isinstance(dataset_source, str):\n",
        "            if \"/\" in dataset_source:  # Hugging Face dataset\n",
        "                print(f\"Loading HF dataset: {dataset_source}\")\n",
        "                self.hf_dataset = load_dataset(dataset_source, split=\"train\")\n",
        "                self.data_type = \"hf\"\n",
        "            else:  # Google Drive path\n",
        "                print(f\"Loading from Drive: {dataset_source}\")\n",
        "                self.load_from_drive(dataset_source)\n",
        "                self.data_type = \"drive\"\n",
        "        else:  # List of paths\n",
        "            self.image_paths = dataset_source\n",
        "            self.data_type = \"paths\"\n",
        "\n",
        "    def load_from_drive(self, drive_path):\n",
        "        \"\"\"Load images from Google Drive folder\"\"\"\n",
        "        import glob\n",
        "        self.image_paths = glob.glob(f\"{drive_path}/**/*.jpg\", recursive=True)\n",
        "        self.image_paths += glob.glob(f\"{drive_path}/**/*.png\", recursive=True)\n",
        "        print(f\"Found {len(self.image_paths)} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.data_type == \"hf\":\n",
        "            return len(self.hf_dataset)\n",
        "        else:\n",
        "            return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image based on source type\n",
        "        if self.data_type == \"hf\":\n",
        "            item = self.hf_dataset[idx]\n",
        "            image = item[\"image\"]\n",
        "            caption = item.get(\"text\", item.get(\"caption\", \"\"))\n",
        "        else:\n",
        "            image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "            # Try to load caption from .txt file with same name\n",
        "            txt_path = self.image_paths[idx].rsplit(\".\", 1)[0] + \".txt\"\n",
        "            try:\n",
        "                with open(txt_path, \"r\") as f:\n",
        "                    caption = f.read().strip()\n",
        "            except:\n",
        "                caption = \"\"\n",
        "\n",
        "        # Preprocess image\n",
        "        image = self.preprocess_image(image)\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"caption\": caption\n",
        "        }\n",
        "\n",
        "    def preprocess_image(self, image):\n",
        "        \"\"\"Preprocess image with resolution bucketing\"\"\"\n",
        "        # Calculate aspect ratio bucket\n",
        "        w, h = image.size\n",
        "        aspect_ratio = w / h\n",
        "\n",
        "        # Define buckets (common aspect ratios)\n",
        "        buckets = [\n",
        "            (1024, 1024),  # 1:1\n",
        "            (1152, 896),   # 9:7\n",
        "            (896, 1152),   # 7:9\n",
        "            (1216, 832),   # 3:2\n",
        "            (832, 1216),   # 2:3\n",
        "        ]\n",
        "\n",
        "        # Find closest bucket\n",
        "        target_w, target_h = min(buckets,\n",
        "            key=lambda b: abs(b[0]/b[1] - aspect_ratio))\n",
        "\n",
        "        # Resize and crop\n",
        "        if self.center_crop:\n",
        "            # Resize to slightly larger than target\n",
        "            scale = max(target_w / w, target_h / h)\n",
        "            new_w, new_h = int(w * scale), int(h * scale)\n",
        "            image = image.resize((new_w, new_h), Image.LANCZOS)\n",
        "\n",
        "            # Center crop to target\n",
        "            left = (new_w - target_w) // 2\n",
        "            top = (new_h - target_h) // 2\n",
        "            image = image.crop((left, top, left + target_w, top + target_h))\n",
        "        else:\n",
        "            image = image.resize((target_w, target_h), Image.LANCZOS)\n",
        "\n",
        "        return image\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "train_dataset = SD35Dataset(DATASET_NAME, resolution=1024)\n",
        "print(f\"âœ“ Dataset loaded: {len(train_dataset)} samples\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c431edad"
      },
      "source": [
        "## 3.2 Dataset Visualization\n",
        "\n",
        "Verify that images and captions are loading correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40411dc2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_batch(dataset, num_samples=4):\n",
        "    \"\"\"Display sample images with captions\"\"\"\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(20, 5))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        sample = dataset[i]\n",
        "        axes[i].imshow(sample[\"image\"])\n",
        "        axes[i].set_title(sample[\"caption\"][:50] + \"...\", fontsize=10)\n",
        "        axes[i].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_batch(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c60c7af"
      },
      "source": [
        "## 4. Training Strategy Selection\n",
        "\n",
        "### Option 1: LoRA (Low-Rank Adaptation) â­ RECOMMENDED\n",
        "**VRAM**: 10-16 GB\n",
        "**Speed**: Fast\n",
        "**Quality**: Excellent for style/subject fine-tuning\n",
        "**Use Case**: Most scenarios\n",
        "\n",
        "- Trains low-rank decomposition matrices (typically rank 4-128)\n",
        "- Only 0.5-5% of original parameters\n",
        "- Minimal quality loss for targeted adaptations\n",
        "- Easy to merge/share\n",
        "\n",
        "### Option 2: Partial Fine-tuning\n",
        "**VRAM**: 20-40 GB\n",
        "**Speed**: Medium\n",
        "**Quality**: Better for drastic style changes\n",
        "**Use Case**: Significant distribution shift\n",
        "\n",
        "- Fine-tune only transformer blocks or text encoders\n",
        "- More expressive than LoRA\n",
        "- Requires more VRAM and time\n",
        "\n",
        "### Option 3: Full Fine-tuning\n",
        "**VRAM**: 40+ GB\n",
        "**Speed**: Slow\n",
        "**Quality**: Maximum control\n",
        "**Use Case**: Large datasets, new domains\n",
        "\n",
        "- Not recommended for Colab due to VRAM limits\n",
        "\n",
        "---\n",
        "\n",
        "**We'll implement LoRA training with automatic VRAM optimization.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f873a50"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Training hyperparameters optimized for Colab\"\"\"\n",
        "\n",
        "    # Model\n",
        "    model_id: str = MODEL_ID\n",
        "\n",
        "    # LoRA settings\n",
        "    lora_rank: int = 16  # 4-128, higher = more expressive\n",
        "    lora_alpha: int = 16  # Scaling factor, usually = rank\n",
        "    lora_dropout: float = 0.0\n",
        "    target_modules: list = None  # Auto-detect transformer blocks\n",
        "\n",
        "    # Training\n",
        "    num_epochs: int = 10\n",
        "    train_batch_size: int = 1  # Per GPU\n",
        "    gradient_accumulation_steps: int = 4  # Effective batch = 4\n",
        "    learning_rate: float = 1e-4\n",
        "    lr_scheduler: str = \"cosine\"  # or \"constant\", \"linear\"\n",
        "    warmup_steps: int = 100\n",
        "\n",
        "    # Optimization\n",
        "    mixed_precision: str = \"fp16\"  # or \"bf16\" for A100\n",
        "    gradient_checkpointing: bool = True\n",
        "    use_8bit_adam: bool = True  # bitsandbytes optimizer\n",
        "    max_grad_norm: float = 1.0\n",
        "\n",
        "    # Data\n",
        "    resolution: int = 1024\n",
        "    dataloader_num_workers: int = 2\n",
        "\n",
        "    # Checkpointing\n",
        "    save_steps: int = 250\n",
        "    checkpointing_steps: int = 500\n",
        "    output_dir: str = OUTPUT_DIR\n",
        "\n",
        "    # Logging\n",
        "    logging_steps: int = 10\n",
        "    report_to: str = \"tensorboard\"  # or \"wandb\"\n",
        "\n",
        "    # Validation\n",
        "    validation_prompt: str = \"a beautiful landscape with mountains\"\n",
        "    num_validation_images: int = 2\n",
        "    validation_steps: int = 250\n",
        "\n",
        "# Auto-adjust based on VRAM\n",
        "config = TrainingConfig()\n",
        "\n",
        "if vram_tier == \"low\":\n",
        "    config.train_batch_size = 1\n",
        "    config.gradient_accumulation_steps = 8\n",
        "    config.lora_rank = 8\n",
        "    config.resolution = 768\n",
        "    print(\"âš™ Using low-VRAM settings\")\n",
        "elif vram_tier == \"medium\":\n",
        "    config.train_batch_size = 1\n",
        "    config.gradient_accumulation_steps = 4\n",
        "    config.lora_rank = 16\n",
        "    print(\"âš™ Using medium-VRAM settings\")\n",
        "else:\n",
        "    config.train_batch_size = 2\n",
        "    config.gradient_accumulation_steps = 2\n",
        "    config.lora_rank = 32\n",
        "    print(\"âš™ Using high-VRAM settings\")\n",
        "\n",
        "print(f\"Effective batch size: {config.train_batch_size * config.gradient_accumulation_steps}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6482e55a"
      },
      "source": [
        "## 5.1 Model Loading with Memory Optimization\n",
        "\n",
        "Load SD3.5 with quantization and gradient checkpointing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcf9f484"
      },
      "source": [
        "from diffusers import StableDiffusion3Pipeline, FlowMatchEulerDiscreteScheduler\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "def load_sd35_for_training(model_id, config):\n",
        "    \"\"\"Load SD3.5 with memory optimizations\"\"\"\n",
        "\n",
        "    print(\"Loading Stable Diffusion 3.5...\")\n",
        "\n",
        "    # Load pipeline with optimizations\n",
        "    pipe = StableDiffusion3Pipeline.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16 if config.mixed_precision == \"fp16\" else torch.bfloat16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True,\n",
        "    )\n",
        "\n",
        "    # Enable memory optimizations\n",
        "    if config.gradient_checkpointing:\n",
        "        pipe.transformer.enable_gradient_checkpointing()\n",
        "        print(\"âœ“ Gradient checkpointing enabled\")\n",
        "\n",
        "    # Enable xformers for memory-efficient attention\n",
        "    try:\n",
        "        pipe.enable_xformers_memory_efficient_attention()\n",
        "        print(\"âœ“ xFormers enabled\")\n",
        "    except:\n",
        "        print(\"âš  xFormers not available\")\n",
        "\n",
        "    # Move to GPU\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Load model\n",
        "pipeline = load_sd35_for_training(config.model_id, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b65ffaa7"
      },
      "source": [
        "## 5.2 LoRA Configuration\n",
        "\n",
        "Configure LoRA adapters for the MMDiT transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dab31d3f"
      },
      "source": [
        "def setup_lora(pipeline, config):\n",
        "    \"\"\"Apply LoRA to SD3.5 transformer\"\"\"\n",
        "\n",
        "    # Configure LoRA for transformer blocks\n",
        "    lora_config = LoraConfig(\n",
        "        r=config.lora_rank,\n",
        "        lora_alpha=config.lora_alpha,\n",
        "        init_lora_weights=\"gaussian\",\n",
        "        target_modules=[\n",
        "            \"to_q\", \"to_k\", \"to_v\", \"to_out.0\",  # Attention layers\n",
        "            \"proj_in\", \"proj_out\",  # Projection layers\n",
        "        ],\n",
        "        lora_dropout=config.lora_dropout,\n",
        "    )\n",
        "\n",
        "    # Apply LoRA to transformer\n",
        "    pipeline.transformer = get_peft_model(pipeline.transformer, lora_config)\n",
        "\n",
        "    # Print trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in pipeline.transformer.parameters() if p.requires_grad)\n",
        "    all_params = sum(p.numel() for p in pipeline.transformer.parameters())\n",
        "\n",
        "    print(f\"âœ“ LoRA configured\")\n",
        "    print(f\"  Trainable params: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%%)\")\n",
        "    print(f\"  Total params: {all_params:,}\")\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "# Setup LoRA\n",
        "pipeline = setup_lora(pipeline, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c3d982a"
      },
      "source": [
        "## 5.3 Training Loop Implementation\n",
        "\n",
        "Full training loop with accelerate integration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "178bd209"
      },
      "source": [
        "from accelerate import Accelerator\n",
        "from accelerate.utils import ProjectConfiguration\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn.functional as F\n",
        "from diffusers.optimization import get_scheduler\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "def collate_fn(examples):\n",
        "    \"\"\"Custom collate function for batching\"\"\"\n",
        "    images = [example[\"image\"] for example in examples]\n",
        "    captions = [example[\"caption\"] for example in examples]\n",
        "    return {\"images\": images, \"captions\": captions}\n",
        "\n",
        "def train_sd35_lora(pipeline, train_dataset, config):\n",
        "    \"\"\"Complete training loop for SD3.5 LoRA\"\"\"\n",
        "\n",
        "    # Initialize accelerator\n",
        "    accelerator_project_config = ProjectConfiguration(\n",
        "        project_dir=config.output_dir,\n",
        "        logging_dir=f\"{config.output_dir}/logs\"\n",
        "    )\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        log_with=config.report_to,\n",
        "        project_config=accelerator_project_config,\n",
        "    )\n",
        "\n",
        "    # Create dataloader\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.train_batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=config.dataloader_num_workers,\n",
        "    )\n",
        "\n",
        "    # Prepare optimizer\n",
        "    if config.use_8bit_adam:\n",
        "        optimizer_cls = bnb.optim.AdamW8bit\n",
        "    else:\n",
        "        optimizer_cls = torch.optim.AdamW\n",
        "\n",
        "    optimizer = optimizer_cls(\n",
        "        pipeline.transformer.parameters(),\n",
        "        lr=config.learning_rate,\n",
        "        betas=(0.9, 0.999),\n",
        "        weight_decay=1e-2,\n",
        "        eps=1e-8,\n",
        "    )\n",
        "\n",
        "    # Prepare scheduler\n",
        "    lr_scheduler = get_scheduler(\n",
        "        config.lr_scheduler,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=config.warmup_steps * config.gradient_accumulation_steps,\n",
        "        num_training_steps=len(train_dataloader) * config.num_epochs,\n",
        "    )\n",
        "\n",
        "    # Prepare with accelerator\n",
        "    pipeline.transformer, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        pipeline.transformer, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    global_step = 0\n",
        "    progress_bar = tqdm(\n",
        "        range(config.num_epochs * len(train_dataloader)),\n",
        "        disable=not accelerator.is_local_main_process,\n",
        "    )\n",
        "    progress_bar.set_description(\"Training\")\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        pipeline.transformer.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(pipeline.transformer):\n",
        "                # Encode images to latents\n",
        "                images = [img for img in batch[\"images\"]]\n",
        "                captions = batch[\"captions\"]\n",
        "\n",
        "                # Process images through VAE\n",
        "                latents = []\n",
        "                for img in images:\n",
        "                    # Convert PIL to tensor\n",
        "                    img_tensor = torch.tensor(\n",
        "                        np.array(img).transpose(2, 0, 1)\n",
        "                    ).unsqueeze(0).float() / 255.0\n",
        "                    img_tensor = img_tensor.to(accelerator.device, dtype=pipeline.vae.dtype)\n",
        "\n",
        "                    # Encode\n",
        "                    with torch.no_grad():\n",
        "                        latent = pipeline.vae.encode(img_tensor).latent_dist.sample()\n",
        "                        latent = latent * pipeline.vae.config.scaling_factor\n",
        "                    latents.append(latent)\n",
        "\n",
        "                latents = torch.cat(latents, dim=0)\n",
        "\n",
        "                # Sample noise and timesteps\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "\n",
        "                # Rectified flow uses timesteps in [0, 1]\n",
        "                timesteps = torch.rand(bsz, device=latents.device)\n",
        "\n",
        "                # Add noise (linear interpolation for rectified flow)\n",
        "                noisy_latents = (1 - timesteps.view(-1, 1, 1, 1)) * latents + \\\n",
        "                               timesteps.view(-1, 1, 1, 1) * noise\n",
        "\n",
        "                # Encode text\n",
        "                with torch.no_grad():\n",
        "                    prompt_embeds, pooled_prompt_embeds = pipeline.encode_prompt(\n",
        "                        prompt=captions,\n",
        "                        prompt_2=captions,\n",
        "                        prompt_3=captions,\n",
        "                        device=accelerator.device,\n",
        "                    )\n",
        "\n",
        "                # Predict the noise (velocity for rectified flow)\n",
        "                model_pred = pipeline.transformer(\n",
        "                    hidden_states=noisy_latents,\n",
        "                    timestep=timesteps,\n",
        "                    encoder_hidden_states=prompt_embeds,\n",
        "                    pooled_projections=pooled_prompt_embeds,\n",
        "                    return_dict=False,\n",
        "                )[0]\n",
        "\n",
        "                # Rectified flow loss: predict velocity (noise - latents)\n",
        "                target = noise - latents\n",
        "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "\n",
        "                # Backprop\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                if accelerator.sync_gradients:\n",
        "                    accelerator.clip_grad_norm_(pipeline.transformer.parameters(), config.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Logging\n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "\n",
        "                if global_step % config.logging_steps == 0:\n",
        "                    logs = {\n",
        "                        \"loss\": loss.detach().item(),\n",
        "                        \"lr\": lr_scheduler.get_last_lr()[0],\n",
        "                        \"step\": global_step,\n",
        "                    }\n",
        "                    progress_bar.set_postfix(**logs)\n",
        "                    accelerator.log(logs, step=global_step)\n",
        "\n",
        "                # Save checkpoint\n",
        "                if global_step % config.checkpointing_steps == 0:\n",
        "                    if accelerator.is_main_process:\n",
        "                        save_path = f\"{config.output_dir}/checkpoint-{global_step}\"\n",
        "                        accelerator.save_state(save_path)\n",
        "                        print(f\"âœ“ Checkpoint saved: {save_path}\")\n",
        "\n",
        "                # Generate validation images\n",
        "                if global_step % config.validation_steps == 0:\n",
        "                    if accelerator.is_main_process:\n",
        "                        generate_validation_images(\n",
        "                            pipeline, config, global_step, accelerator\n",
        "                        )\n",
        "\n",
        "    # Save final LoRA weights\n",
        "    accelerator.wait_for_everyone()\n",
        "    if accelerator.is_main_process:\n",
        "        pipeline.transformer.save_pretrained(f\"{config.output_dir}/final_lora\")\n",
        "        print(f\"âœ“ Final LoRA saved: {config.output_dir}/final_lora\")\n",
        "\n",
        "    accelerator.end_training()\n",
        "\n",
        "def generate_validation_images(pipeline, config, step, accelerator):\n",
        "    \"\"\"Generate validation images during training\"\"\"\n",
        "    pipeline.transformer.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        images = pipeline(\n",
        "            prompt=config.validation_prompt,\n",
        "            num_inference_steps=28,\n",
        "            guidance_scale=7.0,\n",
        "            num_images_per_prompt=config.num_validation_images,\n",
        "        ).images\n",
        "\n",
        "    # Save images\n",
        "    for i, img in enumerate(images):\n",
        "        img.save(f\"{config.output_dir}/validation-{step}-{i}.png\")\n",
        "\n",
        "    pipeline.transformer.train()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Start training\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "train_sd35_lora(pipeline, train_dataset, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00dcce0a"
      },
      "source": [
        "## 6.1 Loading Trained Model\n",
        "\n",
        "Load the base model and merge LoRA weights for inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30eb9b20"
      },
      "source": [
        "def load_trained_model(base_model_id, lora_path, device=\"cuda\"):\n",
        "    \"\"\"Load base model and merge LoRA weights\"\"\"\n",
        "\n",
        "    print(\"Loading model for inference...\")\n",
        "\n",
        "    # Load base pipeline\n",
        "    pipe = StableDiffusion3Pipeline.from_pretrained(\n",
        "        base_model_id,\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True,\n",
        "    )\n",
        "\n",
        "    # Load and merge LoRA weights\n",
        "    pipe.load_lora_weights(lora_path)\n",
        "\n",
        "    # Enable optimizations\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "    pipe.to(device)\n",
        "\n",
        "    print(\"âœ“ Model loaded successfully\")\n",
        "    return pipe\n",
        "\n",
        "# Load trained model\n",
        "inference_pipe = load_trained_model(\n",
        "    config.model_id,\n",
        "    f\"{config.output_dir}/final_lora\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3a07bff"
      },
      "source": [
        "## 6.2 Image Generation\n",
        "\n",
        "Generate images with various samplers and settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12d91450"
      },
      "source": [
        "def generate_images(\n",
        "    pipe,\n",
        "    prompt,\n",
        "    negative_prompt=\"blurry, bad quality, distorted\",\n",
        "    num_images=4,\n",
        "    guidance_scale=7.0,\n",
        "    num_inference_steps=28,\n",
        "    height=1024,\n",
        "    width=1024,\n",
        "    seed=None,\n",
        "):\n",
        "    \"\"\"Generate images with SD3.5\"\"\"\n",
        "\n",
        "    if seed is not None:\n",
        "        generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "    else:\n",
        "        generator = None\n",
        "\n",
        "    images = pipe(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        num_images_per_prompt=num_images,\n",
        "        generator=generator,\n",
        "    ).images\n",
        "\n",
        "    return images\n",
        "\n",
        "# Example generation\n",
        "prompt = \"a serene Japanese garden with cherry blossoms, koi pond, and traditional architecture, highly detailed\"\n",
        "images = generate_images(inference_pipe, prompt, num_images=2, seed=42)\n",
        "\n",
        "# Display\n",
        "fig, axes = plt.subplots(1, len(images), figsize=(15, 8))\n",
        "for i, img in enumerate(images):\n",
        "    if len(images) == 1:\n",
        "        axes.imshow(img)\n",
        "        axes.axis(\"off\")\n",
        "    else:\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].axis(\"off\")\n",
        "plt.suptitle(prompt[:100])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f24362b5"
      },
      "source": [
        "## 6.3 Advanced Sampling\n",
        "\n",
        "Compare different schedulers and settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50e75093"
      },
      "source": [
        "from diffusers import (\n",
        "    FlowMatchEulerDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DDIMScheduler,\n",
        ")\n",
        "\n",
        "def compare_schedulers(pipe, prompt, schedulers_config):\n",
        "    \"\"\"Compare different noise schedulers\"\"\"\n",
        "\n",
        "    results = {}\n",
        "    original_scheduler = pipe.scheduler\n",
        "\n",
        "    for name, scheduler_cls, steps in schedulers_config:\n",
        "        print(f\"Testing {name}...\")\n",
        "        pipe.scheduler = scheduler_cls.from_config(pipe.scheduler.config)\n",
        "\n",
        "        images = generate_images(\n",
        "            pipe, prompt,\n",
        "            num_images=1,\n",
        "            num_inference_steps=steps,\n",
        "            seed=42\n",
        "        )\n",
        "        results[name] = images[0]\n",
        "\n",
        "    pipe.scheduler = original_scheduler\n",
        "    return results\n",
        "\n",
        "# Test different schedulers\n",
        "schedulers = [\n",
        "    (\"Euler (Default)\", FlowMatchEulerDiscreteScheduler, 28),\n",
        "    (\"DPM++ (Fast)\", DPMSolverMultistepScheduler, 20),\n",
        "    (\"DDIM (Quality)\", DDIMScheduler, 50),\n",
        "]\n",
        "\n",
        "test_prompt = \"a cyberpunk city at night, neon lights, rain, cinematic\"\n",
        "scheduler_results = compare_schedulers(inference_pipe, test_prompt, schedulers)\n",
        "\n",
        "# Display comparison\n",
        "fig, axes = plt.subplots(1, len(scheduler_results), figsize=(18, 6))\n",
        "for i, (name, img) in enumerate(scheduler_results.items()):\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(name)\n",
        "    axes[i].axis(\"off\")\n",
        "plt.suptitle(test_prompt[:100])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}